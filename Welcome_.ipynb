{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Satyam-kumar-jha-03/Accident_identifiers/blob/main/Welcome_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define the variable dataset_dir and assign it the correct path to the directory containing the image dataset.\n",
        "# The user needs to replace 'path/to/your/dataset' with the actual path.\n",
        "dataset_dir = r\"\n",
        "\\Downloads\\datasets\"\n",
        "\n",
        "# Check if the directory specified by dataset_dir exists using os.path.exists(). If it doesn't exist, print an informative error message and stop.\n",
        "if not os.path.exists(dataset_dir):\n",
        "    print(f\"Error: Directory '{dataset_dir}' not found. Please ensure the dataset is placed in this directory.\")\n",
        "else:\n",
        "    # Define image parameters (assuming these were defined in previous cells)\n",
        "    # img_height = 128\n",
        "    # img_width = 128\n",
        "    # batch_size = 32\n",
        "\n",
        "    # Create ImageDataGenerator for training and validation\n",
        "    # Rescale pixel values to [0, 1] and specify a validation split\n",
        "    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        validation_split=0.2 # Use 20% of the data for validation\n",
        "    )\n",
        "\n",
        "    # Create training generator\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        dataset_dir,\n",
        "        target_size=(img_height, img_width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary', # Assuming binary classification (AI vs Real)\n",
        "        subset='training'\n",
        "    )\n",
        "\n",
        "    # Create validation generator\n",
        "    validation_generator = train_datagen.flow_from_directory(\n",
        "        dataset_dir,\n",
        "        target_size=(img_height, img_width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary',\n",
        "        subset='validation'\n",
        "    )\n",
        "\n",
        "    # Print a confirmation message indicating that the image data has been loaded and preprocessed for training and validation.\n",
        "    print(\"Image data loaded and preprocessed for training and validation.\")"
      ],
      "metadata": {
        "id": "4ZJBH5yTW6iE",
        "outputId": "82880df2-7e90-4aae-9b9e-a8004b32bbbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Directory '.\\Downloads\\datasets' not found. Please ensure the dataset is placed in this directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ltau93iNQDUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "wJYz_NIgQDW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10  # Define the number of epochs for training\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator\n",
        ")"
      ],
      "metadata": {
        "id": "zufldM1QQDZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "# Assuming the user has now provided the correct path to the dataset directory.\n",
        "# Replace 'path/to/your/dataset' with the actual path if it's not already set correctly.\n",
        "# dataset_dir = 'path/to/your/dataset' # Uncomment and set if necessary\n",
        "\n",
        "# Check if the directory specified by dataset_dir exists. If it doesn't exist, print an informative error message and stop.\n",
        "if not os.path.exists(dataset_dir):\n",
        "    print(f\"Error: Directory '{dataset_dir}' not found. Please ensure the dataset is placed in this directory.\")\n",
        "else:\n",
        "    # Create ImageDataGenerator for training and validation\n",
        "    # Rescale pixel values to [0, 1] and specify a validation split\n",
        "    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        validation_split=0.2 # Use 20% of the data for validation\n",
        "    )\n",
        "\n",
        "    # Create training generator\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        dataset_dir,\n",
        "        target_size=(img_height, img_width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary', # Assuming binary classification (AI vs Real)\n",
        "        subset='training'\n",
        "    )\n",
        "\n",
        "    # Create validation generator\n",
        "    validation_generator = train_datagen.flow_from_directory(\n",
        "        dataset_dir,\n",
        "        target_size=(img_height, img_width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary',\n",
        "        subset='validation'\n",
        "    )\n",
        "\n",
        "    # Print a confirmation message indicating that the image data has been loaded and preprocessed for training and validation.\n",
        "    print(\"Image data loaded and preprocessed for training and validation.\")\n",
        "\n",
        "    # Now that the generators are created, train the model\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        epochs=epochs,\n",
        "        validation_data=validation_generator\n",
        "    )"
      ],
      "metadata": {
        "id": "pWt4QAatQDc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "# Re-create ImageDataGenerator for training and validation\n",
        "# Rescale pixel values to [0, 1] and specify a validation split\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2 # Use 20% of the data for validation\n",
        ")\n",
        "\n",
        "# Create training generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary', # Assuming binary classification (AI vs Real)\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "# Create validation generator\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Print a confirmation message indicating that the image data has been loaded and preprocessed for training and validation.\n",
        "print(\"Image data loaded and preprocessed for training and validation.\")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator\n",
        ")"
      ],
      "metadata": {
        "id": "pTHLJz-fQDfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "# 1. Check if the dataset_dir variable exists and is not empty.\n",
        "try:\n",
        "    # Check if dataset_dir is defined and not None or empty\n",
        "    if not dataset_dir:\n",
        "        raise NameError(\"dataset_dir is empty or not set.\")\n",
        "except NameError:\n",
        "    print(\"Error: dataset_dir variable is not set. Please set the variable with the correct path to the dataset directory.\")\n",
        "else:\n",
        "    # 2. If dataset_dir is set, check if the directory specified by dataset_dir exists.\n",
        "    if not os.path.exists(dataset_dir):\n",
        "        print(f\"Error: Directory '{dataset_dir}' not found. Please ensure the dataset is placed in this directory.\")\n",
        "    else:\n",
        "        # 3. If the dataset_dir is set and the directory exists, re-create the ImageDataGenerator for training and validation.\n",
        "        train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "            rescale=1./255,\n",
        "            validation_split=0.2 # Use 20% of the data for validation\n",
        "        )\n",
        "\n",
        "        # 4. Create the training generator.\n",
        "        train_generator = train_datagen.flow_from_directory(\n",
        "            dataset_dir,\n",
        "            target_size=(img_height, img_width),\n",
        "            batch_size=batch_size,\n",
        "            class_mode='binary', # Assuming binary classification (AI vs Real)\n",
        "            subset='training'\n",
        "        )\n",
        "\n",
        "        # 5. Create the validation generator.\n",
        "        validation_generator = train_datagen.flow_from_directory(\n",
        "            dataset_dir,\n",
        "            target_size=(img_height, img_width),\n",
        "            batch_size=batch_size,\n",
        "            class_mode='binary',\n",
        "            subset='validation'\n",
        "        )\n",
        "\n",
        "        # 6. Print a confirmation message.\n",
        "        print(\"Image data loaded and preprocessed for training and validation.\")\n",
        "\n",
        "        # 7. Train the model.\n",
        "        history = model.fit(\n",
        "            train_generator,\n",
        "            epochs=epochs,\n",
        "            validation_data=validation_generator\n",
        "        )\n",
        "\n",
        "# Evaluate the model on the validation data\n",
        "loss, accuracy = model.evaluate(validation_generator)\n",
        "\n",
        "# Print the evaluation results\n",
        "print(f\"Validation Loss: {loss:.4f}\")\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "R_3ogtVlQDh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 1. Check if the dataset_dir variable exists and is not empty.\n",
        "# Using try-except to catch NameError if dataset_dir is not defined at all.\n",
        "try:\n",
        "    # Check if dataset_dir is defined and not None or empty\n",
        "    if not dataset_dir:\n",
        "        raise NameError(\"dataset_dir is empty or not set.\")\n",
        "except NameError:\n",
        "    print(\"Error: dataset_dir variable is not set. Please set the variable with the correct path to the dataset directory.\")\n",
        "else:\n",
        "    # 2. If dataset_dir is set, check if the directory specified by dataset_dir exists using os.path.exists().\n",
        "    if not os.path.exists(dataset_dir):\n",
        "        print(f\"Error: Directory '{dataset_dir}' not found. Please ensure the dataset is placed in this directory.\")\n",
        "    else:\n",
        "        # 3. If the dataset_dir is set and the directory exists, re-create the ImageDataGenerator for training and validation.\n",
        "        # Assuming tf is imported in a previous cell.\n",
        "        train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "            rescale=1./255,\n",
        "            validation_split=0.2 # Use 20% of the data for validation\n",
        "        )\n",
        "\n",
        "        # 4. Create the training generator using train_datagen.flow_from_directory.\n",
        "        # Assuming img_height, img_width, and batch_size are defined in previous cells.\n",
        "        train_generator = train_datagen.flow_from_directory(\n",
        "            dataset_dir,\n",
        "            target_size=(img_height, img_width),\n",
        "            batch_size=batch_size,\n",
        "            class_mode='binary', # Assuming binary classification (AI vs Real)\n",
        "            subset='training'\n",
        "        )\n",
        "\n",
        "        # 5. Create the validation generator using train_datagen.flow_from_directory.\n",
        "        validation_generator = train_datagen.flow_from_directory(\n",
        "            dataset_dir,\n",
        "            target_size=(img_height, img_width),\n",
        "            batch_size=batch_size,\n",
        "            class_mode='binary',\n",
        "            subset='validation'\n",
        "        )\n",
        "\n",
        "        # 6. Print a confirmation message.\n",
        "        print(\"Image data loaded and preprocessed for training and validation.\")\n",
        "\n",
        "        # 7. Evaluate the model on the validation data using model.evaluate().\n",
        "        # Assuming model is defined and compiled in previous cells.\n",
        "        # Use the 'epochs' variable to get the number of steps for evaluation.\n",
        "        loss, accuracy = model.evaluate(validation_generator, steps=epochs)\n",
        "\n",
        "        # 8. Store the returned loss and accuracy values in variables. (Already done in the previous step)\n",
        "\n",
        "        # 9. Print the validation loss and accuracy, formatted to 4 decimal places.\n",
        "        print(f\"Validation Loss: {loss:.4f}\")\n",
        "        print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "l1qQasZ5QDk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def predict_single_image(image_path, model, img_height, img_width, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Predicts whether a single image is AI-generated or real using the trained model.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): The path to the image file.\n",
        "        model (tf.keras.Model): The trained TensorFlow model.\n",
        "        img_height (int): The target height of the image.\n",
        "        img_width (int): The target width of the image.\n",
        "        threshold (float): The probability threshold for classification.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the predicted class ('AI-generated' or 'Real')\n",
        "               and the raw prediction value (a float between 0 and 1).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the image\n",
        "        img = tf.keras.preprocessing.image.load_img(\n",
        "            image_path, target_size=(img_height, img_width)\n",
        "        )\n",
        "\n",
        "        # Convert to NumPy array\n",
        "        img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "\n",
        "        # Rescale to 0-1\n",
        "        img_array = img_array / 255.0\n",
        "\n",
        "        # Expand dimensions to add batch size\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "        # Make prediction\n",
        "        predictions = model.predict(img_array)\n",
        "        score = predictions[0][0] # Get the scalar prediction\n",
        "\n",
        "        # Interpret prediction\n",
        "        if score > threshold:\n",
        "            predicted_class = 'AI-generated'\n",
        "        else:\n",
        "            predicted_class = 'Real'\n",
        "\n",
        "        return predicted_class, score\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        return \"Error: Image file not found.\", None\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {e}\", None\n",
        "\n",
        "# Example usage (replace 'path/to/your/new_image.jpg' with an actual image path)\n",
        "# Note: The 'model' variable is assumed to be trained from previous steps.\n",
        "# If the model training failed, this example will not run successfully.\n",
        "# You need a successfully trained 'model' object.\n",
        "example_image_path = 'path/to/your/new_image.jpg' # Placeholder path\n",
        "# Check if the model exists before attempting to predict\n",
        "if 'model' in locals() and isinstance(model, tf.keras.Model):\n",
        "    predicted_class, confidence = predict_single_image(example_image_path, model, img_height, img_width)\n",
        "\n",
        "    if confidence is not None:\n",
        "        print(f\"The image at {example_image_path} is predicted to be: {predicted_class}\")\n",
        "        print(f\"Confidence score: {confidence:.4f}\")\n",
        "else:\n",
        "    print(\"Error: Model is not trained or not available.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "cjuKmhn_QDnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AHmQH9-SQDqn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}